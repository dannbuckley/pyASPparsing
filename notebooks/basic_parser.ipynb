{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum.verify(enum.CONTINUOUS, enum.UNIQUE)\n",
    "class TokenType(enum.Enum):\n",
    "    SYMBOL = 1\n",
    "    IDENTIFIER = 2\n",
    "    LITERAL_STRING = 3\n",
    "    LITERAL_INT = 4\n",
    "    LITERAL_HEX = 5\n",
    "    LITERAL_OCT = 6\n",
    "    LITERAL_FLOAT = 7\n",
    "    LITERAL_DATE = 8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    \"\"\"Contains Token type and associated portion of source code\"\"\"\n",
    "\n",
    "    token_type: TokenType\n",
    "    token_src: slice = slice(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<% Response.Write(\"Hello, world!\") %>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_code = \"\"\"<% Response.Write(\"Hello, world!\") %>\"\"\"\n",
    "test_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type                      Source\n",
      "------------------------- ---------------\n",
      "TokenType.SYMBOL          <\n",
      "TokenType.SYMBOL          %\n",
      "TokenType.IDENTIFIER      Response\n",
      "TokenType.SYMBOL          .\n",
      "TokenType.IDENTIFIER      Write\n",
      "TokenType.SYMBOL          (\n",
      "TokenType.LITERAL_STRING  \"Hello, world!\"\n",
      "TokenType.SYMBOL          )\n",
      "TokenType.SYMBOL          %\n",
      "TokenType.SYMBOL          >\n",
      "\n",
      "Type                      Source\n",
      "------------------------- ---------------\n",
      "TokenType.LITERAL_HEX     &H7f\n",
      "TokenType.LITERAL_HEX     &H10abCf33&\n",
      "\n",
      "Type                      Source\n",
      "------------------------- ---------------\n",
      "TokenType.LITERAL_OCT     &12736\n",
      "TokenType.LITERAL_OCT     &171&\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def yield_token(codeblock: str) -> typing.Generator[Token, None, None]:\n",
    "    \"\"\"Iteratively tokenize ASP code string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    codeblock : str\n",
    "        Classic ASP source code\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Token\n",
    "        The next available token in the codeblock\n",
    "    \"\"\"\n",
    "    code_iter: typing.Iterator[str] = iter(codeblock)\n",
    "    # preload first character\n",
    "    pos_char: typing.Optional[str] = next(code_iter, None)\n",
    "    pos_idx: int = 0\n",
    "    # global test_iter, pos_char, pos_idx\n",
    "\n",
    "    def _advance_pos() -> bool:\n",
    "        \"\"\"Advance to the next position in the ASP code string\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Returns True if codeblock iterator not exhausted\n",
    "        \"\"\"\n",
    "        # modify state of enclosing function\n",
    "        nonlocal code_iter, pos_char, pos_idx\n",
    "        pos_char = next(code_iter, None)\n",
    "        pos_idx += 1\n",
    "        return pos_char is not None\n",
    "\n",
    "    # main tokenizer loop\n",
    "    # base case: exit at end of string (i.e., when iterator exhausted)\n",
    "    while pos_char is not None:\n",
    "        # determine token type\n",
    "        if pos_char.isspace():\n",
    "            # consume and ignore whitespace\n",
    "            while _advance_pos() and pos_char.isspace():\n",
    "                pass\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char.isalpha():\n",
    "            # basic example identifier: [a-zA-Z][a-zA-Z0-9]*\n",
    "            start_iden = pos_idx  # save starting index of identifier for later\n",
    "            # goto end of identifier\n",
    "            while _advance_pos() and pos_char.isalnum():\n",
    "                pass\n",
    "            yield Token(TokenType.IDENTIFIER, slice(start_iden, pos_idx))\n",
    "            del start_iden\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char == '\"':\n",
    "            # string literal\n",
    "            start_str = pos_idx  # save starting index of string literal for later\n",
    "            # goto end of string\n",
    "            found_dbl_quote = False\n",
    "            terminated = False\n",
    "            while _advance_pos():\n",
    "                if pos_char == '\"' and not found_dbl_quote:\n",
    "                    found_dbl_quote = True\n",
    "                    continue\n",
    "                if found_dbl_quote:\n",
    "                    if pos_char == '\"':\n",
    "                        # quote is escaped ('\"\"'), keep iterating\n",
    "                        found_dbl_quote = False\n",
    "                        continue\n",
    "                    else:\n",
    "                        # end of string literal, stop iterating\n",
    "                        terminated = True\n",
    "                        break\n",
    "            if not terminated:\n",
    "                raise RuntimeError(\n",
    "                    \"Tokenizer error: Expected ending '\\\"' for string literal, but reached end of code string\"\n",
    "                )\n",
    "            yield Token(TokenType.LITERAL_STRING, slice(start_str, pos_idx))\n",
    "            del found_dbl_quote, terminated\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char.isnumeric():\n",
    "            # int or float literal\n",
    "            pass\n",
    "        elif pos_char == \"&\":\n",
    "            # hex or oct literal\n",
    "            _advance_pos()  # consume '&'\n",
    "            if pos_char == \"H\":\n",
    "                # hex literal\n",
    "                _advance_pos()  # consume 'H'\n",
    "                # need at least one hexadecimal digit\n",
    "                if not (pos_char.isnumeric() or pos_char in \"abcdefABCDEF\"):\n",
    "                    raise RuntimeError(\n",
    "                        f\"Tokenizer error: Expected at least one hexadecimal digit after '&H', but found '{pos_char}' instead\"\n",
    "                    )\n",
    "                start_hex = pos_idx - 2  # include '&H' in Token object\n",
    "                while _advance_pos() and (\n",
    "                    pos_char.isnumeric() or pos_char in \"abcdefABCDEF\"\n",
    "                ):\n",
    "                    pass\n",
    "                # check for optional '&' at end\n",
    "                if pos_char == \"&\":\n",
    "                    _advance_pos()  # consume\n",
    "                yield Token(TokenType.LITERAL_HEX, slice(start_hex, pos_idx))\n",
    "                del start_hex\n",
    "                # already at next character, don't advance\n",
    "            else:\n",
    "                # oct literal\n",
    "                # need at least one octal digit\n",
    "                if not pos_char in \"01234567\":\n",
    "                    raise RuntimeError(\n",
    "                        f\"Tokenizer error: Expected at least one octal digit after '&', but found '{pos_char}' instead\"\n",
    "                    )\n",
    "                start_oct = pos_idx - 1  # include '&' in Token object\n",
    "                while _advance_pos() and pos_char in \"01234567\":\n",
    "                    pass\n",
    "                # check for optional '&' at end\n",
    "                if pos_char == \"&\":\n",
    "                    _advance_pos()  # consume\n",
    "                yield Token(TokenType.LITERAL_OCT, slice(start_oct, pos_idx))\n",
    "                del start_oct\n",
    "                # already at next character, don't advance\n",
    "        elif pos_char == \"#\":\n",
    "            # date literal\n",
    "            pass\n",
    "        else:\n",
    "            # other token type\n",
    "            yield Token(TokenType.SYMBOL, slice(pos_idx, pos_idx + 1))\n",
    "            _advance_pos()  # move past current token\n",
    "\n",
    "\n",
    "def print_tokenizer_output(codeblock: str):\n",
    "    print(\"Type\".ljust(25), \"Source\")\n",
    "    print(\"-\" * 25, \"-\" * 15)\n",
    "    for tok in yield_token(codeblock):\n",
    "        print(str(tok.token_type).ljust(25), codeblock[tok.token_src])\n",
    "    print()  # add extra spacing\n",
    "\n",
    "\n",
    "print_tokenizer_output(test_code)\n",
    "print_tokenizer_output(\"&H7f &H10abCf33&\")  # hex literal\n",
    "print_tokenizer_output(\"&12736 &171&\")  # oct literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
