{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum.verify(enum.CONTINUOUS, enum.UNIQUE)\n",
    "class TokenType(enum.Enum):\n",
    "    SYMBOL = 1\n",
    "    IDENTIFIER = 2\n",
    "    LITERAL_STRING = 3\n",
    "    LITERAL_INT = 4\n",
    "    LITERAL_HEX = 5\n",
    "    LITERAL_OCT = 6\n",
    "    LITERAL_FLOAT = 7\n",
    "    LITERAL_DATE = 8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    \"\"\"Contains Token type and associated portion of source code\"\"\"\n",
    "\n",
    "    token_type: TokenType\n",
    "    token_src: slice = slice(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source code:\n",
      "<% Response.Write(\"Hello, world!\") %>\n",
      "| Type                        | Source\n",
      "---------------------------------------------\n",
      "| TokenType.SYMBOL            | <\n",
      "| TokenType.SYMBOL            | %\n",
      "| TokenType.IDENTIFIER        | Response\n",
      "| TokenType.SYMBOL            | .\n",
      "| TokenType.IDENTIFIER        | Write\n",
      "| TokenType.SYMBOL            | (\n",
      "| TokenType.LITERAL_STRING    | \"Hello, world!\"\n",
      "| TokenType.SYMBOL            | )\n",
      "| TokenType.SYMBOL            | %\n",
      "| TokenType.SYMBOL            | >\n",
      "\n",
      "Source code:\n",
      "&H7f &H10abCf33&\n",
      "| Type                        | Source\n",
      "---------------------------------------------\n",
      "| TokenType.LITERAL_HEX       | &H7f\n",
      "| TokenType.LITERAL_HEX       | &H10abCf33&\n",
      "\n",
      "Source code:\n",
      "&12736 &171&\n",
      "| Type                        | Source\n",
      "---------------------------------------------\n",
      "| TokenType.LITERAL_OCT       | &12736\n",
      "| TokenType.LITERAL_OCT       | &171&\n",
      "\n",
      "Source code:\n",
      "\"This is a valid string\"\n",
      "| Type                        | Source\n",
      "---------------------------------------------\n",
      "| TokenType.LITERAL_STRING    | \"This is a valid string\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def yield_token(codeblock: str) -> typing.Generator[Token, None, None]:\n",
    "    \"\"\"Iteratively tokenize ASP code string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    codeblock : str\n",
    "        Classic ASP source code\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Token\n",
    "        The next available token in the codeblock\n",
    "    \"\"\"\n",
    "    code_iter: typing.Iterator[str] = iter(codeblock)\n",
    "    # preload first character\n",
    "    pos_char: typing.Optional[str] = next(code_iter, None)\n",
    "    pos_idx: int = 0\n",
    "    # global test_iter, pos_char, pos_idx\n",
    "\n",
    "    def _advance_pos() -> bool:\n",
    "        \"\"\"Advance to the next position in the ASP code string\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Returns True if codeblock iterator not exhausted\n",
    "        \"\"\"\n",
    "        # modify state of enclosing function\n",
    "        nonlocal code_iter, pos_char, pos_idx\n",
    "        pos_char = next(code_iter, None)\n",
    "        pos_idx += 1\n",
    "        return pos_char is not None\n",
    "\n",
    "    # main tokenizer loop\n",
    "    # base case: exit at end of string (i.e., when iterator exhausted)\n",
    "    while pos_char is not None:\n",
    "        # determine token type\n",
    "        if pos_char.isspace():\n",
    "            # consume and ignore whitespace\n",
    "            while _advance_pos() and pos_char.isspace():\n",
    "                pass\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char.isalpha():\n",
    "            # basic example identifier: [a-zA-Z][a-zA-Z0-9]*\n",
    "            start_iden = pos_idx  # save starting index of identifier for later\n",
    "            # goto end of identifier\n",
    "            while _advance_pos() and pos_char.isalnum():\n",
    "                pass\n",
    "            yield Token(TokenType.IDENTIFIER, slice(start_iden, pos_idx))\n",
    "            del start_iden\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char == '\"':\n",
    "            # string literal\n",
    "            start_str = pos_idx  # save starting index of string literal for later\n",
    "            # goto end of string\n",
    "            found_dbl_quote = False\n",
    "            terminated = False\n",
    "            while _advance_pos():\n",
    "                if pos_char == '\"' and not found_dbl_quote:\n",
    "                    found_dbl_quote = True\n",
    "                    continue\n",
    "                if found_dbl_quote:\n",
    "                    if pos_char == '\"':\n",
    "                        # quote is escaped ('\"\"'), keep iterating\n",
    "                        found_dbl_quote = False\n",
    "                        continue\n",
    "                    else:\n",
    "                        # end of string literal, stop iterating\n",
    "                        terminated = True\n",
    "                        break\n",
    "            if not found_dbl_quote and not terminated:\n",
    "                raise RuntimeError(\n",
    "                    \"Tokenizer error: Expected ending '\\\"' for string literal, but reached end of code string\"\n",
    "                )\n",
    "            yield Token(TokenType.LITERAL_STRING, slice(start_str, pos_idx))\n",
    "            del start_str, found_dbl_quote, terminated\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char.isnumeric():\n",
    "            # int or float literal\n",
    "            pass\n",
    "        elif pos_char == \"&\":\n",
    "            # hex or oct literal\n",
    "            _advance_pos()  # consume '&'\n",
    "            if pos_char == \"H\":\n",
    "                # hex literal\n",
    "                _advance_pos()  # consume 'H'\n",
    "                # need at least one hexadecimal digit\n",
    "                if not (pos_char.isnumeric() or pos_char.casefold() in \"abcdef\"):\n",
    "                    raise RuntimeError(\n",
    "                        f\"Tokenizer error: Expected at least one hexadecimal digit after '&H', but found '{pos_char}' instead\"\n",
    "                    )\n",
    "                start_hex = pos_idx - 2  # include '&H' in Token object\n",
    "                while _advance_pos() and (\n",
    "                    pos_char.isnumeric() or pos_char.casefold() in \"abcdef\"\n",
    "                ):\n",
    "                    pass\n",
    "                # check for optional '&' at end\n",
    "                if pos_char == \"&\":\n",
    "                    _advance_pos()  # consume\n",
    "                yield Token(TokenType.LITERAL_HEX, slice(start_hex, pos_idx))\n",
    "                del start_hex\n",
    "                # already at next character, don't advance\n",
    "            else:\n",
    "                # oct literal\n",
    "                # need at least one octal digit\n",
    "                if not pos_char in \"01234567\":\n",
    "                    raise RuntimeError(\n",
    "                        f\"Tokenizer error: Expected at least one octal digit after '&', but found '{pos_char}' instead\"\n",
    "                    )\n",
    "                start_oct = pos_idx - 1  # include '&' in Token object\n",
    "                while _advance_pos() and pos_char in \"01234567\":\n",
    "                    pass\n",
    "                # check for optional '&' at end\n",
    "                if pos_char == \"&\":\n",
    "                    _advance_pos()  # consume\n",
    "                yield Token(TokenType.LITERAL_OCT, slice(start_oct, pos_idx))\n",
    "                del start_oct\n",
    "                # already at next character, don't advance\n",
    "        elif pos_char == \"#\":\n",
    "            # date literal\n",
    "            pass\n",
    "        else:\n",
    "            # other token type\n",
    "            yield Token(TokenType.SYMBOL, slice(pos_idx, pos_idx + 1))\n",
    "            _advance_pos()  # move past current token\n",
    "\n",
    "\n",
    "def print_tokenizer_output(codeblock: str):\n",
    "    print(\"Source code:\", codeblock, sep=\"\\n\")\n",
    "    print(\"| Type\".ljust(30), \"| Source\", sep=\"\")\n",
    "    print(\"-\" * 45)\n",
    "    for tok in yield_token(codeblock):\n",
    "        print(\"| \" + str(tok.token_type).ljust(28), \"| \" + codeblock[tok.token_src], sep=\"\")\n",
    "    print()  # add extra spacing\n",
    "\n",
    "\n",
    "print_tokenizer_output(\"\"\"<% Response.Write(\"Hello, world!\") %>\"\"\")\n",
    "print_tokenizer_output(\"&H7f &H10abCf33&\")  # hex literal\n",
    "print_tokenizer_output(\"&12736 &171&\")  # oct literal\n",
    "print_tokenizer_output('\"This is a valid string\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
