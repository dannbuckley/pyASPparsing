{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum.verify(enum.CONTINUOUS, enum.UNIQUE)\n",
    "class TokenType(enum.Enum):\n",
    "    SYMBOL = 1\n",
    "    IDENTIFIER = 2\n",
    "    LITERAL_STRING = 3\n",
    "    LITERAL_INT = 4\n",
    "    LITERAL_HEX = 5\n",
    "    LITERAL_OCT = 6\n",
    "    LITERAL_FLOAT = 7\n",
    "    LITERAL_DATE = 8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    \"\"\"Contains Token type and associated portion of source code\"\"\"\n",
    "\n",
    "    token_type: TokenType\n",
    "    token_src: slice = slice(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<% Response.Write(\"Hello, world!\") %>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_code = \"\"\"<% Response.Write(\"Hello, world!\") %>\"\"\"\n",
    "test_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type                      Source\n",
      "------------------------- ---------------\n",
      "TokenType.SYMBOL          <\n",
      "TokenType.SYMBOL          %\n",
      "TokenType.IDENTIFIER      Response\n",
      "TokenType.SYMBOL          .\n",
      "TokenType.IDENTIFIER      Write\n",
      "TokenType.SYMBOL          (\n",
      "TokenType.LITERAL_STRING  \"Hello, world!\"\n",
      "TokenType.SYMBOL          )\n",
      "TokenType.SYMBOL          %\n",
      "TokenType.SYMBOL          >\n"
     ]
    }
   ],
   "source": [
    "test_iter: typing.Iterator[str] = iter(test_code)\n",
    "# preload first character\n",
    "pos_char: typing.Optional[str] = next(test_iter, None)\n",
    "pos_idx: int = 0\n",
    "\n",
    "\n",
    "def advance_pos():\n",
    "    \"\"\"Advance to the next position in the ASP code string\"\"\"\n",
    "    global test_iter, pos_char, pos_idx\n",
    "    pos_char = next(test_iter, None)\n",
    "    pos_idx += 1\n",
    "    return pos_char is not None\n",
    "\n",
    "\n",
    "def yield_token() -> typing.Generator[Token, None, None]:\n",
    "    \"\"\"Iteratively tokenize ASP code string\"\"\"\n",
    "    # modify the global state\n",
    "    # (should move this to a class)\n",
    "    global test_iter, pos_char, pos_idx\n",
    "\n",
    "    # main tokenizer loop\n",
    "    # base case: exit at end of string (i.e., when iterator exhausted)\n",
    "    while pos_char is not None:\n",
    "        # determine token type\n",
    "        if pos_char.isspace():\n",
    "            # consume and ignore whitespace\n",
    "            while advance_pos() and pos_char.isspace():\n",
    "                pass\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char.isalpha():\n",
    "            # basic example identifier: [a-zA-Z][a-zA-Z0-9]*\n",
    "            start_iden = pos_idx  # save starting index of identifier for later\n",
    "            # goto end of identifier\n",
    "            while advance_pos() and pos_char.isalnum():\n",
    "                pass\n",
    "            yield Token(TokenType.IDENTIFIER, slice(start_iden, pos_idx))\n",
    "            del start_iden\n",
    "            # already at next character, don't advance\n",
    "        elif pos_char == '\"':\n",
    "            # string literal\n",
    "            start_str = pos_idx  # save starting index of string literal for later\n",
    "            # goto end of string\n",
    "            found_dbl_quote = False\n",
    "            terminated = False\n",
    "            while advance_pos():\n",
    "                if pos_char == '\"' and not found_dbl_quote:\n",
    "                    found_dbl_quote = True\n",
    "                    continue\n",
    "                if found_dbl_quote:\n",
    "                    if pos_char == '\"':\n",
    "                        # quote is escaped ('\"\"'), keep iterating\n",
    "                        found_dbl_quote = False\n",
    "                        continue\n",
    "                    else:\n",
    "                        # end of string literal, stop iterating\n",
    "                        terminated = True\n",
    "                        break\n",
    "            if not terminated:\n",
    "                raise RuntimeError(\n",
    "                    \"Tokenizer error: Expected ending '\\\"' for string literal, but reached end of code string\"\n",
    "                )\n",
    "            yield Token(TokenType.LITERAL_STRING, slice(start_str, pos_idx))\n",
    "            del found_dbl_quote, terminated\n",
    "            # already at next character, don't advance\n",
    "        else:\n",
    "            # other token type\n",
    "            yield Token(TokenType.SYMBOL, slice(pos_idx, pos_idx + 1))\n",
    "            advance_pos()  # move past current token\n",
    "\n",
    "\n",
    "print(\"Type\".ljust(25), \"Source\")\n",
    "print(\"-\" * 25, \"-\" * 15)\n",
    "for tok in yield_token():\n",
    "    print(str(tok.token_type).ljust(25), test_code[tok.token_src])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
